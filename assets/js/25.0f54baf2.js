(window.webpackJsonp=window.webpackJsonp||[]).push([[25],{300:function(e,n,t){"use strict";t.r(n);var i=t(10),r=Object(i.a)({},(function(){var e=this,n=e._self._c;return n("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[n("h1",{attrs:{id:"python爬虫"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#python爬虫"}},[e._v("#")]),e._v(" Python爬虫")]),e._v(" "),n("p",[e._v("环境的搭建")]),e._v(" "),n("ol",[n("li",[n("p",[e._v("安装python: "),n("a",{attrs:{href:"https://www.cnblogs.com/zjazn/p/15364795.html",title:"https://www.cnblogs.com/zjazn/p/15364795.html",target:"_blank",rel:"noopener noreferrer"}},[e._v("https://www.cnblogs.com/zjazn/p/15364795.html"),n("OutboundLink")],1)])]),e._v(" "),n("li",[n("p",[e._v("安装环境：")])])]),e._v(" "),n("div",{staticClass:"language-sh= extra-class"},[n("pre",{pre:!0,attrs:{class:"language-text"}},[n("code",[e._v('python3 -m venv venv_name\ncd venv_name/bin\nsource activate  //进入独立环境\npip install beautifulsoup4  //安装beautifulsoup4\npython3 //直接python3\nfrom bs4 import BeautifulSoup //导入beautifulsoup4 不报错则安装成功\nexit() #退出python3\n\n//如何要使用编辑器\nmkdir ../../scraping_std\ncd  ../../scraping_std\npython -m  idlelib //启动编辑器-"new File" - Run\n\n')])])]),n("p",[e._v("在专业编辑器上使用专用编辑器")]),e._v(" "),n("p",[n("img",{attrs:{src:"https://cdn.jsdelivr.net/gh/18476305640/typora@master/image/16342170963681634217096336.png",alt:""}})]),e._v(" "),n("h2",{attrs:{id:"爬虫学习"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#爬虫学习"}},[e._v("#")]),e._v(" 爬虫学习")]),e._v(" "),n("p",[e._v("BeautifulSoup在线文档")]),e._v(" "),n("p",[e._v("1、[正式实例]抓取果核剥壳文章")]),e._v(" "),n("div",{staticClass:"language-python== extra-class"},[n("pre",{pre:!0,attrs:{class:"language-text"}},[n("code",[e._v('import requests\nfrom bs4 import BeautifulSoup\n#下载网络文件\ndef download(file_url, save_ful_name):\n    file_stream = requests.get(file_url, stream=True)\n    with open(save_ful_name, \'wb\') as file:\n        for block in file_stream.iter_content(1024):\n            file.write(block)\n#将文本保存为本地文件\ndef write_to_local(file_path, content):\n    paper = open(file_path, mode=\'w\',encoding=\'utf-8\')\n    paper.write(content)\n\n#将页面url html转为对象\ndef page_to_obj(page_url):\n    headers = {\n        \'User-Agent\': \'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.71 Safari/537.36\',\n        \'Accept\': \'*/*\',\n        \'Accept-Language\': \'zh-CN,zh;q=0.9\',\n        \'Accept-Encoding\': \'gzip, deflate, br\',\n        \'DNT\': \'1\',\n        \'Connection\': \'keep-alive\'\n    }\n    page = requests.get(page_url, headers=headers)\n    return BeautifulSoup(page.content, \'html.parser\')\n#进度视图\ndef percentage_view(val):\n    sum_block = 0\n    view = ""\n    if val > 1:\n        sum_block = (val.__floor__()/10).__floor__()\n    else:\n        sum_block = ((val*100).__floor__()/10).__floor__()\n    view += "["\n    for block in range(1, sum_block+1):\n        view += "#"\n    for blank in range(sum_block+1, 11):\n        view += "_"\n    view += "]"\n    return view\n\nif __name__ == "__main__":\n    count_obj = page_to_obj(\'https://www.ghxi.com/category/all\')\n    count_txt = count_obj.find(\'li\', {"class": "disabled"}).get_text()\n    start_page_number = int(count_txt.split("/")[0])\n    end_page_number = int(count_txt.split("/")[1])\n    for i in range(start_page_number, end_page_number):\n        print("【开始抓取{}/{}页，当前进度{} 】".format(i,end_page_number, percentage_view(i/end_page_number)))\n        page_url = \'https://www.ghxi.com/category/all/page/{}\'.format(i)\n        item_page_obj = page_to_obj(page_url)\n        page_items = item_page_obj.findAll(\'h2\', {"class": "item-title"})\n        for page_item in page_items:\n            page_item_title = page_item.find(\'a\').get_text().strip().replace("/", "或").replace("|", "或").replace("?", " ").replace("*", "~")\n            page_item_link = page_item.find(\'a\').attrs[\'href\']\n            page_item_content = page_to_obj(page_item_link).find("div", {"class": "entry-content"}).get_text()\n            write_to_local(page_item_title+".txt", page_item_content)\n            print("{}\\t\\t{}".format(page_item_title, page_item_link))\n')])])]),n("p",[e._v("2、[正式实例]  抓取乐愚蓝奏云链接[使用到下面的函数]")]),e._v(" "),n("div",{staticClass:"language-python== extra-class"},[n("pre",{pre:!0,attrs:{class:"language-text"}},[n("code",[e._v('def climb_pages_item():\n    share_file = open(\'leyu_share_links.txt\', \'a+\', encoding=\'utf-8\')\n    def link_write_local(in_stream, links):\n        for link in links:\n            print("正在写入文件...")\n            in_stream.write("{}\\n".format(json.dumps(link)))\n            in_stream.flush()\n            print("[end]")\n\n    #1、如何获取总页面数  返回总页数\n    page_count = 527\n    for n in range(0,page_count):\n        print("正在抓取{}/{}".format(n+1,page_count)+percentage_view((n+1)/page_count))\n        #如何根据页数，返回要跳转的链接  返回跳转链接\n        tar_page_link = "https://bbs.leyuz.net/f/ruanjian?pn={}&od=0".format(n+1)\n        link_obj = page_to_obj(tar_page_link)\n        #如何从页面中获取item对象； 返回item_obj对象\n        items_obj = link_obj.findAll("a", {"class": "tag-title"})\n        for item_obj in items_obj:\n            #item_obj  返回item_link\n            item_link = "https://bbs.leyuz.net"+item_obj.attrs[\'href\']\n            #从item详情页中获取content\n            item_content = page_to_obj(item_link).find(\'article\', {"class": "blog-main"})\n            if item_content != None:\n                item_content = item_content.get_text()\n            else:\n                item_content = \'\'\n            #从item详情页中获取评论（数组对象）\n            items_comment = page_to_obj(item_link).find(\'article\', {"class": "am-comment"})\n            item_sum_text = \'\'\n            if items_comment != None:\n                for item_comment in items_comment:\n                    item_sum_text += item_comment.get_text()+"\\n"\n            item_all_content = item_content+"\\n"+item_sum_text\n            #筛选特征分享链接\n            _share =  get_share_link(item_all_content, \'lanzou\');\n            link_write_local(share_file, _share)\n            print(_share)\n    share_file.close()\n')])])]),n("p",[e._v("10、[正式函数] 函数")]),e._v(" "),n("div",{staticClass:"language-python= extra-class"},[n("pre",{pre:!0,attrs:{class:"language-text"}},[n("code",[e._v("import requests\nfrom bs4 import BeautifulSoup\nimport re\nfrom PyPDF2 import PdfFileReader, PdfFileWriter\n\nimport json\nimport os\nfrom tkinter import *\nfrom tkinter import ttk\n\n\n#下载网络文件\ndef download(file_url, save_ful_name):\n    file_stream = requests.get(file_url, stream=True)\n    with open(save_ful_name, 'wb') as file:\n        for block in file_stream.iter_content(1024):\n            file.write(block)\n#将文本保存为本地文件\ndef write_to_local(file_path, content):\n    paper = open(file_path, mode='w',encoding='utf-8')\n    paper.write(content)\n\n#将页面url html转为对象\ndef page_to_obj(page_url):\n    headers = {\n        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.71 Safari/537.36',\n        'Accept': '*/*',\n        'Accept-Language': 'zh-CN,zh;q=0.9',\n        'Accept-Encoding': 'gzip, deflate, br',\n        'DNT': '1',\n        'Connection': 'keep-alive'\n    }\n    page = requests.get(page_url, headers=headers)\n    return BeautifulSoup(page.content, 'html.parser')\n#进度视图\ndef percentage_view(val):\n    sum_block = 0\n    view = \"\"\n    if val > 1:\n        sum_block = (val.__floor__()/10).__floor__()\n    else:\n        sum_block = ((val*100).__floor__()/10).__floor__()\n    view += \"[\"\n    for block in range(1, sum_block+1):\n        view += \"#\"\n    for blank in range(sum_block+1, 11):\n        view += \"_\"\n    view += \"]\"\n    return view\n    \n#操作的字符\ndef ctl_string(ctl_string, re_str, limit ) -> object:\n    if type(limit) == list:\n        #进行搜索模式 参数： 操作的字符串(str)  re(str) [start_index,end_index]\n        pattern = re.compile(r'{}'.format(re_str))\n        if len(limit) == 0:\n            return pattern.findall(ctl_string)\n        else:\n            return pattern.findall(ctl_string, limit[0], limit[1])\n    elif type(limit) == int:\n        #进入替换模式 参数：操作的字符串(str) [re,replace_str] count(int)\n        if limit == 0:\n            return re.sub(re_str[0], re_str[1], ctl_string)\n        else:\n            return re.sub(re_str[0], re_str[1], ctl_string, limit)\n    else:\n        raise RuntimeError('第三个参数有重要作用，传入是一个整数值是替换操作，代表替换的数量，传入一个二值整形数组是查找，是查找的位置范围')\n\n\n\n#将pdf指定的多个的页面转到新的pdf文件中\n#pdf_page_pick(\"ctl.pdf\", [5, 6], to=\"split.pdf\")\ndef pdf_page_pick(pdf_file_path, pages, to):\n    pdf_file_stream = open(r\"{}\".format(pdf_file_path), \"rb\") #要读取的pdf文件\n    pdf_read = PdfFileReader(pdf_file_stream)  #读取pdf为处理对象\n    pdf_write = PdfFileWriter()  #创建pdf写处理对象\n    for n in pages:\n        print(\"当前索引{}\".format(n))\n        pdf_write.addPage(pdf_read.getPage(n-1))  #将要求的页面添加到写的队列中\n    child_pdf_stream = open(r\"{}\".format(to), \"wb\") #创建文件的输出流\n    pdf_write.write(child_pdf_stream) #开始输出\n\n\n#传一html中file name\ndef get_html_file_name(page_text):\n    _name = ctl_string(page_text, \"\\>.+\\.[0-9|A-z]+[^(com)|(con)|\\>|\\;]<\\/\", [])\n    if len(_name) > 0:\n        return _name[0][1:][:-2]\n    else:\n        return None\n        \n#扫描文本字符串有指定的链接并扫描最近可能的四位分享提取码\ndef get_share_link(text_str, features):\n   def clean_key_str(dirty_key):\n       new_key = ctl_string(dirty_key, \"[\\w]{4}\", [])\n       if len(new_key) < 1:\n           return \"\"\n       return new_key[0]\n   #安全访问数组，避免越界访问\n   def security_access_array(arr,index):\n       if len(arr) > index:\n           return arr[index]\n       else:\n           return None\n   #开始准备利用正则获取分享链接信息，并最终加入到一个数组中\n   re = 'http[s]?:\\/\\/[A-z|0-9|\\.|\\/|\\-]+'\n   share_links = ctl_string(text_str, re, [])  #不具有特征的分享链接或全部链接\n   key_seach_between = []\n   for share in share_links:\n       index = text_str.index(share)\n       if(len(key_seach_between) != 0):\n           key_seach_between.insert(len(key_seach_between), index)\n       key_seach_between.insert(len(key_seach_between), index+len(share))\n   key_seach_between.insert(len(key_seach_between),-1)\n   #开始获取分享链接的提取码并且合并分享信息到一个对象数组中\n   shares = [] #全部的分享信息\n   share_info = {'link': \"\", 'key': \"\"}  #分享信息单元\n   point = 0\n   end_index = len(key_seach_between) - 2\n   current_link_index = 0\n   while(point <= end_index):\n        tail_index = 0\n        if key_seach_between[point+1] == -1:\n            tail_index = len(text_str)\n        else:\n            tail_index = key_seach_between[point+1]\n        #print(\"在{}到{}这个范围查找提取码\".format(key_seach_between[point], tail_index ))\n\n        key = security_access_array(ctl_string(text_str, \"[:|：| ]+?[0-9|A-z]{4}[^0-9|^A-z]+?\", [key_seach_between[point], tail_index]), 0)\n        share_info['link'] = share_links[current_link_index]\n        share_info['key'] = clean_key_str(key if (key != None) else '')\n        shares.insert(len(shares), share_info)\n        point += 2\n        current_link_index += 1\n        share_info = {'link': \"\", 'key': \"\"} #重置分享信息\n   #筛选出具有特征的链接\n   false_link_count = 0\n   _shares = [] #具有目标特征的分享链接\n   for _share in shares:\n       try:\n           has_share = _share['link'].index(features)\n       except Exception as e:\n           has_share = None\n           false_link_count += 1\n       if has_share != None:\n           _shares.insert(len(_shares), _share)\n   print(\"扫描完成，一共有：{}个链接，特征链接{}个，假链接{}个！\".format(len(shares), len(shares)-false_link_count, false_link_count))\n   return _shares\n   \n   \n#对文本行进行操作，返回为新行\ndef text_line_ctl(local_file_path, ctl_fun):\n    ##\n    # def _ctl(line, line_n):\n    #     if line_n == 0:\n    #         return line\n    #     return line + \"已作修改！\"\n    #\n    # text_line_ctl(\"leyu_share_links.txt\", _ctl)\n    ##\n    def join(arr, between_str):\n        result = ''\n        length = len(arr)\n        for n in range(0, length):\n            result += (arr[n] + between_str)\n        return result\n    try:\n        r = open(local_file_path, 'r', encoding='utf-8')\n        split = r.read().split(\"\\n\")\n        if split[len(split) - 1] == \"\":\n            del split[len(split) - 1]\n        temp_file = open(\"ctl-{}\".format(local_file_path), 'w+', encoding=\"utf-8\")\n        for n in range(0, len(split)):\n            new_line = ctl_fun(split[n], n)\n\n            temp_file.write(new_line+\"\\n\")\n            temp_file.flush()\n\n            split[n] = new_line\n        new_text = join(split, \"\\n\")\n        w = open(local_file_path, 'w', encoding='utf-8')\n        w.write(new_text)\n    finally:\n        temp_file.close()\n        r.close()\n        w.close()\n   \n#对文本文件的内容进行替换 replace_text_content(\"leyu_share_links.txt\", \"lanzous\", \"lanzoui\")\ndef replace_text_content(file_path,update_before,update_after):\n    file_r = open(file_path, 'r', encoding=\"utf-8\")\n\n    try:\n        new_text_content = ctl_string(file_r.read(), [update_before, update_after], 0)\n        print(new_text_content)\n        file_w = open(file_path, 'w', encoding=\"utf-8\")\n        file_w.write(new_text_content)\n    finally:\n        file_r.close()\n        file_w.close()\n\n")])])]),n("h2",{attrs:{id:"控制浏览器"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#控制浏览器"}},[e._v("#")]),e._v(" 控制浏览器")]),e._v(" "),n("div",{staticClass:"language-python== extra-class"},[n("pre",{pre:!0,attrs:{class:"language-text"}},[n("code",[e._v('from selenium import webdriver\n#需要下载安装 chromedriver.exe\n#下载：https://npm.taobao.org/mirrors/chromedriver\n#文档：https://www.selenium.dev/zh-cn/documentation/overview/\nbrowser = webdriver.Chrome(executable_path=\'./chromedriver.exe\')\ndef tips_and_go(selector):\n    elem = browser.find_element_by_css_selector(selector)\n    print("开始点击：{}".format(elem.text))\n    elem.click()\n    # 等待新标签页完成加载内容\ndef to_last_window():\n    window_ids = browser.window_handles\n    last_id = window_ids[len(window_ids)-1]\n    browser.switch_to.window(last_id)\n    print("当前窗口是：{}".format(browser.current_window_handle))\n\ndef close_window(closed_to):\n    browser.close()\n    window_ids = browser.window_handles\n    browser.switch_to.window(window_ids[closed_to])\ndef end_browser():\n    for id in browser.window_handles:\n        to_last_window()\n        browser.close()\n\nbrowser.get("https://www.baidu.com")\ntips_and_go("#s-top-left > a:nth-child(1)")\nto_last_window()\ntips_and_go(\'#channel-all > div > ul > li:nth-child(3) > a\')\n\nclose_window(closed_to=0)\nprint(browser.current_window_handle)\n\nend_browser()\n')])])])])}),[],!1,null,null,null);n.default=r.exports}}]);