<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Python爬虫 | 肥小猪</title>
    <meta name="generator" content="VuePress 1.9.7">
    <link rel="icon" href="/favicon.ico">
    <meta name="description" content="肥小猪的笔记">
    <meta name="author" content="zhuangjie">
    <meta name="keywords" content="小庄的博客 小庄的笔记 zhuangjie 庄杰">
    
    <link rel="preload" href="/assets/css/0.styles.83bfd168.css" as="style"><link rel="preload" href="/assets/js/app.5c666aae.js" as="script"><link rel="preload" href="/assets/js/2.f4acce4e.js" as="script"><link rel="preload" href="/assets/js/25.0f54baf2.js" as="script"><link rel="prefetch" href="/assets/js/10.7d647c34.js"><link rel="prefetch" href="/assets/js/11.ea5229fb.js"><link rel="prefetch" href="/assets/js/12.b859755e.js"><link rel="prefetch" href="/assets/js/13.94d3b5a2.js"><link rel="prefetch" href="/assets/js/14.ed2f4480.js"><link rel="prefetch" href="/assets/js/15.b3e305e6.js"><link rel="prefetch" href="/assets/js/16.49f8a835.js"><link rel="prefetch" href="/assets/js/17.f4b4e831.js"><link rel="prefetch" href="/assets/js/18.582de590.js"><link rel="prefetch" href="/assets/js/19.c85bd795.js"><link rel="prefetch" href="/assets/js/20.92741434.js"><link rel="prefetch" href="/assets/js/21.b7c8497e.js"><link rel="prefetch" href="/assets/js/22.96b46a67.js"><link rel="prefetch" href="/assets/js/23.0bc030ab.js"><link rel="prefetch" href="/assets/js/24.87f04d87.js"><link rel="prefetch" href="/assets/js/26.1c39a9d2.js"><link rel="prefetch" href="/assets/js/27.31d22914.js"><link rel="prefetch" href="/assets/js/28.fc7f7405.js"><link rel="prefetch" href="/assets/js/29.1e94a8cc.js"><link rel="prefetch" href="/assets/js/3.c8d9910f.js"><link rel="prefetch" href="/assets/js/30.61082fbd.js"><link rel="prefetch" href="/assets/js/31.9e80c088.js"><link rel="prefetch" href="/assets/js/32.8ef6a48c.js"><link rel="prefetch" href="/assets/js/33.3349c1af.js"><link rel="prefetch" href="/assets/js/34.14ace618.js"><link rel="prefetch" href="/assets/js/35.3dca7364.js"><link rel="prefetch" href="/assets/js/36.25d01ad8.js"><link rel="prefetch" href="/assets/js/37.4527c97e.js"><link rel="prefetch" href="/assets/js/38.30b3c9ab.js"><link rel="prefetch" href="/assets/js/39.6f6d8c82.js"><link rel="prefetch" href="/assets/js/4.c72729ae.js"><link rel="prefetch" href="/assets/js/40.6454dbfd.js"><link rel="prefetch" href="/assets/js/41.ba49a8db.js"><link rel="prefetch" href="/assets/js/42.2ec3f8ee.js"><link rel="prefetch" href="/assets/js/43.e40c89c6.js"><link rel="prefetch" href="/assets/js/44.3d45a7aa.js"><link rel="prefetch" href="/assets/js/45.2375de76.js"><link rel="prefetch" href="/assets/js/46.a4cfa405.js"><link rel="prefetch" href="/assets/js/47.ab954289.js"><link rel="prefetch" href="/assets/js/48.4b72016d.js"><link rel="prefetch" href="/assets/js/49.9d06f5f2.js"><link rel="prefetch" href="/assets/js/5.44547aad.js"><link rel="prefetch" href="/assets/js/50.8dc92268.js"><link rel="prefetch" href="/assets/js/51.59e6212e.js"><link rel="prefetch" href="/assets/js/52.adecc9a3.js"><link rel="prefetch" href="/assets/js/53.030d635f.js"><link rel="prefetch" href="/assets/js/54.52060cbc.js"><link rel="prefetch" href="/assets/js/6.18c29f84.js"><link rel="prefetch" href="/assets/js/7.6427b380.js"><link rel="prefetch" href="/assets/js/8.67154bc3.js"><link rel="prefetch" href="/assets/js/9.757fe49c.js">
    <link rel="stylesheet" href="/assets/css/0.styles.83bfd168.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container"><header class="navbar"><div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/" class="home-link router-link-active"><img src="/assets/img/logo.png" alt="肥小猪" class="logo"> <span class="site-name can-hide">肥小猪</span></a> <div class="links"><div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="/note/程序员的自身修养.html" class="nav-link">
  总览
</a></div> <!----></nav></div></header> <div class="sidebar-mask"></div> <aside class="sidebar"><nav class="nav-links"><div class="nav-item"><a href="/note/程序员的自身修养.html" class="nav-link">
  总览
</a></div> <!----></nav>  <ul class="sidebar-links"><li><section class="sidebar-group depth-0"><p class="sidebar-heading open"><span>Python爬虫</span> <!----></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/note/Python%E7%88%AC%E8%99%AB/Python%E7%88%AC%E8%99%AB.html#爬虫学习" class="sidebar-link">爬虫学习</a><ul class="sidebar-sub-headers"></ul></li><li><a href="/note/Python%E7%88%AC%E8%99%AB/Python%E7%88%AC%E8%99%AB.html#控制浏览器" class="sidebar-link">控制浏览器</a><ul class="sidebar-sub-headers"></ul></li></ul></section></li></ul> </aside> <main class="page"> <div class="theme-default-content content__default"><h1 id="python爬虫"><a href="#python爬虫" class="header-anchor">#</a> Python爬虫</h1> <p>环境的搭建</p> <ol><li><p>安装python: <a href="https://www.cnblogs.com/zjazn/p/15364795.html" title="https://www.cnblogs.com/zjazn/p/15364795.html" target="_blank" rel="noopener noreferrer">https://www.cnblogs.com/zjazn/p/15364795.html<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p></li> <li><p>安装环境：</p></li></ol> <div class="language-sh= extra-class"><pre class="language-text"><code>python3 -m venv venv_name
cd venv_name/bin
source activate  //进入独立环境
pip install beautifulsoup4  //安装beautifulsoup4
python3 //直接python3
from bs4 import BeautifulSoup //导入beautifulsoup4 不报错则安装成功
exit() #退出python3

//如何要使用编辑器
mkdir ../../scraping_std
cd  ../../scraping_std
python -m  idlelib //启动编辑器-&quot;new File&quot; - Run

</code></pre></div><p>在专业编辑器上使用专用编辑器</p> <p><img src="https://cdn.jsdelivr.net/gh/18476305640/typora@master/image/16342170963681634217096336.png" alt=""></p> <h2 id="爬虫学习"><a href="#爬虫学习" class="header-anchor">#</a> 爬虫学习</h2> <p>BeautifulSoup在线文档</p> <p>1、[正式实例]抓取果核剥壳文章</p> <div class="language-python== extra-class"><pre class="language-text"><code>import requests
from bs4 import BeautifulSoup
#下载网络文件
def download(file_url, save_ful_name):
    file_stream = requests.get(file_url, stream=True)
    with open(save_ful_name, 'wb') as file:
        for block in file_stream.iter_content(1024):
            file.write(block)
#将文本保存为本地文件
def write_to_local(file_path, content):
    paper = open(file_path, mode='w',encoding='utf-8')
    paper.write(content)

#将页面url html转为对象
def page_to_obj(page_url):
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.71 Safari/537.36',
        'Accept': '*/*',
        'Accept-Language': 'zh-CN,zh;q=0.9',
        'Accept-Encoding': 'gzip, deflate, br',
        'DNT': '1',
        'Connection': 'keep-alive'
    }
    page = requests.get(page_url, headers=headers)
    return BeautifulSoup(page.content, 'html.parser')
#进度视图
def percentage_view(val):
    sum_block = 0
    view = &quot;&quot;
    if val &gt; 1:
        sum_block = (val.__floor__()/10).__floor__()
    else:
        sum_block = ((val*100).__floor__()/10).__floor__()
    view += &quot;[&quot;
    for block in range(1, sum_block+1):
        view += &quot;#&quot;
    for blank in range(sum_block+1, 11):
        view += &quot;_&quot;
    view += &quot;]&quot;
    return view

if __name__ == &quot;__main__&quot;:
    count_obj = page_to_obj('https://www.ghxi.com/category/all')
    count_txt = count_obj.find('li', {&quot;class&quot;: &quot;disabled&quot;}).get_text()
    start_page_number = int(count_txt.split(&quot;/&quot;)[0])
    end_page_number = int(count_txt.split(&quot;/&quot;)[1])
    for i in range(start_page_number, end_page_number):
        print(&quot;【开始抓取{}/{}页，当前进度{} 】&quot;.format(i,end_page_number, percentage_view(i/end_page_number)))
        page_url = 'https://www.ghxi.com/category/all/page/{}'.format(i)
        item_page_obj = page_to_obj(page_url)
        page_items = item_page_obj.findAll('h2', {&quot;class&quot;: &quot;item-title&quot;})
        for page_item in page_items:
            page_item_title = page_item.find('a').get_text().strip().replace(&quot;/&quot;, &quot;或&quot;).replace(&quot;|&quot;, &quot;或&quot;).replace(&quot;?&quot;, &quot; &quot;).replace(&quot;*&quot;, &quot;~&quot;)
            page_item_link = page_item.find('a').attrs['href']
            page_item_content = page_to_obj(page_item_link).find(&quot;div&quot;, {&quot;class&quot;: &quot;entry-content&quot;}).get_text()
            write_to_local(page_item_title+&quot;.txt&quot;, page_item_content)
            print(&quot;{}\t\t{}&quot;.format(page_item_title, page_item_link))
</code></pre></div><p>2、[正式实例]  抓取乐愚蓝奏云链接[使用到下面的函数]</p> <div class="language-python== extra-class"><pre class="language-text"><code>def climb_pages_item():
    share_file = open('leyu_share_links.txt', 'a+', encoding='utf-8')
    def link_write_local(in_stream, links):
        for link in links:
            print(&quot;正在写入文件...&quot;)
            in_stream.write(&quot;{}\n&quot;.format(json.dumps(link)))
            in_stream.flush()
            print(&quot;[end]&quot;)

    #1、如何获取总页面数  返回总页数
    page_count = 527
    for n in range(0,page_count):
        print(&quot;正在抓取{}/{}&quot;.format(n+1,page_count)+percentage_view((n+1)/page_count))
        #如何根据页数，返回要跳转的链接  返回跳转链接
        tar_page_link = &quot;https://bbs.leyuz.net/f/ruanjian?pn={}&amp;od=0&quot;.format(n+1)
        link_obj = page_to_obj(tar_page_link)
        #如何从页面中获取item对象； 返回item_obj对象
        items_obj = link_obj.findAll(&quot;a&quot;, {&quot;class&quot;: &quot;tag-title&quot;})
        for item_obj in items_obj:
            #item_obj  返回item_link
            item_link = &quot;https://bbs.leyuz.net&quot;+item_obj.attrs['href']
            #从item详情页中获取content
            item_content = page_to_obj(item_link).find('article', {&quot;class&quot;: &quot;blog-main&quot;})
            if item_content != None:
                item_content = item_content.get_text()
            else:
                item_content = ''
            #从item详情页中获取评论（数组对象）
            items_comment = page_to_obj(item_link).find('article', {&quot;class&quot;: &quot;am-comment&quot;})
            item_sum_text = ''
            if items_comment != None:
                for item_comment in items_comment:
                    item_sum_text += item_comment.get_text()+&quot;\n&quot;
            item_all_content = item_content+&quot;\n&quot;+item_sum_text
            #筛选特征分享链接
            _share =  get_share_link(item_all_content, 'lanzou');
            link_write_local(share_file, _share)
            print(_share)
    share_file.close()
</code></pre></div><p>10、[正式函数] 函数</p> <div class="language-python= extra-class"><pre class="language-text"><code>import requests
from bs4 import BeautifulSoup
import re
from PyPDF2 import PdfFileReader, PdfFileWriter

import json
import os
from tkinter import *
from tkinter import ttk


#下载网络文件
def download(file_url, save_ful_name):
    file_stream = requests.get(file_url, stream=True)
    with open(save_ful_name, 'wb') as file:
        for block in file_stream.iter_content(1024):
            file.write(block)
#将文本保存为本地文件
def write_to_local(file_path, content):
    paper = open(file_path, mode='w',encoding='utf-8')
    paper.write(content)

#将页面url html转为对象
def page_to_obj(page_url):
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.71 Safari/537.36',
        'Accept': '*/*',
        'Accept-Language': 'zh-CN,zh;q=0.9',
        'Accept-Encoding': 'gzip, deflate, br',
        'DNT': '1',
        'Connection': 'keep-alive'
    }
    page = requests.get(page_url, headers=headers)
    return BeautifulSoup(page.content, 'html.parser')
#进度视图
def percentage_view(val):
    sum_block = 0
    view = &quot;&quot;
    if val &gt; 1:
        sum_block = (val.__floor__()/10).__floor__()
    else:
        sum_block = ((val*100).__floor__()/10).__floor__()
    view += &quot;[&quot;
    for block in range(1, sum_block+1):
        view += &quot;#&quot;
    for blank in range(sum_block+1, 11):
        view += &quot;_&quot;
    view += &quot;]&quot;
    return view
    
#操作的字符
def ctl_string(ctl_string, re_str, limit ) -&gt; object:
    if type(limit) == list:
        #进行搜索模式 参数： 操作的字符串(str)  re(str) [start_index,end_index]
        pattern = re.compile(r'{}'.format(re_str))
        if len(limit) == 0:
            return pattern.findall(ctl_string)
        else:
            return pattern.findall(ctl_string, limit[0], limit[1])
    elif type(limit) == int:
        #进入替换模式 参数：操作的字符串(str) [re,replace_str] count(int)
        if limit == 0:
            return re.sub(re_str[0], re_str[1], ctl_string)
        else:
            return re.sub(re_str[0], re_str[1], ctl_string, limit)
    else:
        raise RuntimeError('第三个参数有重要作用，传入是一个整数值是替换操作，代表替换的数量，传入一个二值整形数组是查找，是查找的位置范围')



#将pdf指定的多个的页面转到新的pdf文件中
#pdf_page_pick(&quot;ctl.pdf&quot;, [5, 6], to=&quot;split.pdf&quot;)
def pdf_page_pick(pdf_file_path, pages, to):
    pdf_file_stream = open(r&quot;{}&quot;.format(pdf_file_path), &quot;rb&quot;) #要读取的pdf文件
    pdf_read = PdfFileReader(pdf_file_stream)  #读取pdf为处理对象
    pdf_write = PdfFileWriter()  #创建pdf写处理对象
    for n in pages:
        print(&quot;当前索引{}&quot;.format(n))
        pdf_write.addPage(pdf_read.getPage(n-1))  #将要求的页面添加到写的队列中
    child_pdf_stream = open(r&quot;{}&quot;.format(to), &quot;wb&quot;) #创建文件的输出流
    pdf_write.write(child_pdf_stream) #开始输出


#传一html中file name
def get_html_file_name(page_text):
    _name = ctl_string(page_text, &quot;\&gt;.+\.[0-9|A-z]+[^(com)|(con)|\&gt;|\;]&lt;\/&quot;, [])
    if len(_name) &gt; 0:
        return _name[0][1:][:-2]
    else:
        return None
        
#扫描文本字符串有指定的链接并扫描最近可能的四位分享提取码
def get_share_link(text_str, features):
   def clean_key_str(dirty_key):
       new_key = ctl_string(dirty_key, &quot;[\w]{4}&quot;, [])
       if len(new_key) &lt; 1:
           return &quot;&quot;
       return new_key[0]
   #安全访问数组，避免越界访问
   def security_access_array(arr,index):
       if len(arr) &gt; index:
           return arr[index]
       else:
           return None
   #开始准备利用正则获取分享链接信息，并最终加入到一个数组中
   re = 'http[s]?:\/\/[A-z|0-9|\.|\/|\-]+'
   share_links = ctl_string(text_str, re, [])  #不具有特征的分享链接或全部链接
   key_seach_between = []
   for share in share_links:
       index = text_str.index(share)
       if(len(key_seach_between) != 0):
           key_seach_between.insert(len(key_seach_between), index)
       key_seach_between.insert(len(key_seach_between), index+len(share))
   key_seach_between.insert(len(key_seach_between),-1)
   #开始获取分享链接的提取码并且合并分享信息到一个对象数组中
   shares = [] #全部的分享信息
   share_info = {'link': &quot;&quot;, 'key': &quot;&quot;}  #分享信息单元
   point = 0
   end_index = len(key_seach_between) - 2
   current_link_index = 0
   while(point &lt;= end_index):
        tail_index = 0
        if key_seach_between[point+1] == -1:
            tail_index = len(text_str)
        else:
            tail_index = key_seach_between[point+1]
        #print(&quot;在{}到{}这个范围查找提取码&quot;.format(key_seach_between[point], tail_index ))

        key = security_access_array(ctl_string(text_str, &quot;[:|：| ]+?[0-9|A-z]{4}[^0-9|^A-z]+?&quot;, [key_seach_between[point], tail_index]), 0)
        share_info['link'] = share_links[current_link_index]
        share_info['key'] = clean_key_str(key if (key != None) else '')
        shares.insert(len(shares), share_info)
        point += 2
        current_link_index += 1
        share_info = {'link': &quot;&quot;, 'key': &quot;&quot;} #重置分享信息
   #筛选出具有特征的链接
   false_link_count = 0
   _shares = [] #具有目标特征的分享链接
   for _share in shares:
       try:
           has_share = _share['link'].index(features)
       except Exception as e:
           has_share = None
           false_link_count += 1
       if has_share != None:
           _shares.insert(len(_shares), _share)
   print(&quot;扫描完成，一共有：{}个链接，特征链接{}个，假链接{}个！&quot;.format(len(shares), len(shares)-false_link_count, false_link_count))
   return _shares
   
   
#对文本行进行操作，返回为新行
def text_line_ctl(local_file_path, ctl_fun):
    ##
    # def _ctl(line, line_n):
    #     if line_n == 0:
    #         return line
    #     return line + &quot;已作修改！&quot;
    #
    # text_line_ctl(&quot;leyu_share_links.txt&quot;, _ctl)
    ##
    def join(arr, between_str):
        result = ''
        length = len(arr)
        for n in range(0, length):
            result += (arr[n] + between_str)
        return result
    try:
        r = open(local_file_path, 'r', encoding='utf-8')
        split = r.read().split(&quot;\n&quot;)
        if split[len(split) - 1] == &quot;&quot;:
            del split[len(split) - 1]
        temp_file = open(&quot;ctl-{}&quot;.format(local_file_path), 'w+', encoding=&quot;utf-8&quot;)
        for n in range(0, len(split)):
            new_line = ctl_fun(split[n], n)

            temp_file.write(new_line+&quot;\n&quot;)
            temp_file.flush()

            split[n] = new_line
        new_text = join(split, &quot;\n&quot;)
        w = open(local_file_path, 'w', encoding='utf-8')
        w.write(new_text)
    finally:
        temp_file.close()
        r.close()
        w.close()
   
#对文本文件的内容进行替换 replace_text_content(&quot;leyu_share_links.txt&quot;, &quot;lanzous&quot;, &quot;lanzoui&quot;)
def replace_text_content(file_path,update_before,update_after):
    file_r = open(file_path, 'r', encoding=&quot;utf-8&quot;)

    try:
        new_text_content = ctl_string(file_r.read(), [update_before, update_after], 0)
        print(new_text_content)
        file_w = open(file_path, 'w', encoding=&quot;utf-8&quot;)
        file_w.write(new_text_content)
    finally:
        file_r.close()
        file_w.close()

</code></pre></div><h2 id="控制浏览器"><a href="#控制浏览器" class="header-anchor">#</a> 控制浏览器</h2> <div class="language-python== extra-class"><pre class="language-text"><code>from selenium import webdriver
#需要下载安装 chromedriver.exe
#下载：https://npm.taobao.org/mirrors/chromedriver
#文档：https://www.selenium.dev/zh-cn/documentation/overview/
browser = webdriver.Chrome(executable_path='./chromedriver.exe')
def tips_and_go(selector):
    elem = browser.find_element_by_css_selector(selector)
    print(&quot;开始点击：{}&quot;.format(elem.text))
    elem.click()
    # 等待新标签页完成加载内容
def to_last_window():
    window_ids = browser.window_handles
    last_id = window_ids[len(window_ids)-1]
    browser.switch_to.window(last_id)
    print(&quot;当前窗口是：{}&quot;.format(browser.current_window_handle))

def close_window(closed_to):
    browser.close()
    window_ids = browser.window_handles
    browser.switch_to.window(window_ids[closed_to])
def end_browser():
    for id in browser.window_handles:
        to_last_window()
        browser.close()

browser.get(&quot;https://www.baidu.com&quot;)
tips_and_go(&quot;#s-top-left &gt; a:nth-child(1)&quot;)
to_last_window()
tips_and_go('#channel-all &gt; div &gt; ul &gt; li:nth-child(3) &gt; a')

close_window(closed_to=0)
print(browser.current_window_handle)

end_browser()
</code></pre></div></div> <footer class="page-edit"><!----> <div class="last-updated"><span class="prefix">更新时间:</span> <span class="time">2022年12月30日星期五下午4点40分</span></div></footer> <!----> </main></div><div class="global-ui"><!----></div></div>
    <script src="/assets/js/app.5c666aae.js" defer></script><script src="/assets/js/2.f4acce4e.js" defer></script><script src="/assets/js/25.0f54baf2.js" defer></script>
  </body>
</html>
